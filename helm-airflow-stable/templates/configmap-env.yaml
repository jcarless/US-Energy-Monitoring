apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ template "airflow.fullname" . }}-env"
  labels:
    app: {{ template "airflow.name" . }}
    chart: {{ template "airflow.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  LOAD_EX: "n"
  TZ: Etc/UTC
  ## Postgres DB configuration
  POSTGRES_HOST: "{{ template "airflow.postgresql.fullname" . }}"
  POSTGRES_PORT: "{{ .Values.postgresql.service.port }}"
  POSTGRES_DB: "{{ .Values.postgresql.postgresDatabase }}"
  {{- if eq .Values.airflow.executor "Celery" }}
  ## Redis DB configuration
  REDIS_HOST: "{{ template "airflow.redis.host" . }}"
  REDIS_PORT: "{{ .Values.redis.master.port }}"
  AIRFLOW__CELERY__FLOWER_URL_PREFIX: "{{ .Values.flower.urlPrefix }}"
  AIRFLOW__CELERY__WORKER_CONCURRENCY: "{{ .Values.workers.celery.instances }}"
  ## Flower PORT
  FLOWER_PORT: "5555"
  # For backwards compat with AF < 1.10, CELERY_CONCURRENCY got renamed to WORKER_CONCURRENCY
  AIRFLOW__CELERY__CELERY_CONCURRENCY: "{{ .Values.workers.celery.instances }}"
  {{- end }}
  {{- if eq .Values.airflow.executor "Kubernetes" }} 
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY: "{{ .Values.airflow.image.repository }}"
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG: "{{ .Values.airflow.image.tag }}"
  AIRFLOW__KUBERNETES__CATCHUP_BY_DEFAULT: "False"
  AIRFLOW__KUBERNETES__ENV_FROM_CONFIGMAP_REF: "{{ include "airflow.fullname" . }}-env"
  AIRFLOW__KUBERNETES__DAGS_IN_IMAGE: "True"
  AIRFLOW__KUBERNETES__DELETE_WORKER_PODS: "True"
  AIRFLOW__KUBERNETES__NAMESPACE: "{{ .Release.Namespace }}"
  AIRFLOW__SCHEDULER__STATSD_ON: "False"
  AIRFLOW__CORE__DAG_PROCESSOR_MANAGER_LOG_LOCATION: /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log
  # AIRFLOW__WEBSERVER__ACCESS_LOGFILE: /usr/local/airflow/logs/web.log
  # AIRFLOW__WEBSERVER__ERROR_LOGFILE: /usr/local/airflow/logs/web_error.log
  {{- end }}
  # Configure puckel's docker-airflow entrypoint
  EXECUTOR: "{{ .Values.airflow.executor }}"
  AIRFLOW__CORE__FERNET_KEY: "{{ .Values.airflow.fernetKey }}"
  DO_WAIT_INITDB: "false"
  # Remote Logging
  AWS_ACCESS_KEY_ID: "{{ .Values.aws.access_key }}"
  AWS_SECRET_ACCESS_KEY: "{{ .Values.aws.secret_key }}"
  AIRFLOW__CORE__REMOTE_LOGGING: "{{ .Values.logs.remote_logging.enable }}"
  AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER: "{{ .Values.logs.remote_logging.base_folder }}"
  AIRFLOW__CORE__REMOTE_LOG_CONN_ID: "{{ .Values.logs.remote_logging.conn_id }}"
  encrypt_s3_logs: "False"
  ## Custom Airflow settings
  AIRFLOW__CORE__DONOT_PICKLE: "{{ .Values.dags.doNotPickle }}"
  AIRFLOW__CORE__DAGS_FOLDER: "{{ .Values.dags.path }}"
  AIRFLOW__CORE__BASE_LOG_FOLDER: "{{ .Values.logs.path }}"
  AIRFLOW__CORE__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "{{ printf "%s/%s" .Values.logs.path "dag_processor_manager/dag_processor_manager.log" }}"
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "{{ printf "%s/%s" .Values.logs.path "scheduler" }}"
  AIRFLOW__WEBSERVER__BASE_URL: "{{ .Values.web.baseUrl }}"
  # Disabling XCom pickling for forward compatibility
  AIRFLOW__CORE__ENABLE_XCOM_PICKLING: "false"
  # Note: changing `Values.airflow.config` won't change the configmap checksum and so won't make
  # the pods to restart
  {{- range $setting, $option := .Values.airflow.config }}
  {{ $setting }}: "{{ $option }}"
  {{- end }}
